# -*- coding: utf-8 -*-
"""Keyword Suggessions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z8WxqrIq9fup-0FHtjFbcqpsbuvEuxSX

**GAME DESCRIPTION ANALYSIS AND KEYWORD SUGGESTIONS**

The Game data scrapped from the playstore is imported into the notebook. 
The required libraries are imported like pandas to work with the datasets, matplotlib to do the visualizations and draft the graphs.
"""

# Commented out IPython magic to ensure Python compatibility.
# libraries for basic operations and plots
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.cm as cm
# %matplotlib inline
import seaborn as sb

"""Below we have other libraries imported that work with K-Means clustering model."""

#libraries used for modelling
from sklearn.cluster import MiniBatchKMeans
from sklearn.cluster import KMeans

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

"""The text analysis is done using the vectorizers that tend to operate on the description given to extract importand words from it."""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

"""Some of the other libraries supported by the python to work on text are impoted here that uses NLP (Natural language procesing) and NLTK (Natural language tool kit)"""

#libraries for NLP operations
import re
from bs4 import BeautifulSoup

import string
string.punctuation

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('words')
stopword=nltk.corpus.stopwords.words('english')
all_words = set(nltk.corpus.words.words())

from collections import Counter
from nltk.stem.porter import PorterStemmer

porter = PorterStemmer()

"""The Google drive is connected tp the notebook and the files available in the drive were mounted using the below code."""

from google.colab import drive
drive.mount('/content/gdrive/')

cd /content/gdrive

ls

"""The batch file containing gaming data is imported at first from the drive."""

# /content/gdrive/My Drive/Batchfile1.csv
df1=pd.read_csv('My Drive/Batchfile1.csv')
df1.head()

"""The gaming dataset is imported from google drive and the column names were extracted."""

df2=pd.read_csv('My Drive/Gaming.csv')
del df2['keywords']
fea=df2.columns
fea

"""As the first dataframe, df1 doesnt posses column names, the feature names of the gamin dataset, df2 were assigned to df1."""

df1.columns=fea

df2.head()
# print(len(df1))
# len(df2)

"""The two dataframes each consisting of two batch files of datasets are merged together into a dataframe-rawset."""

rawset= pd.concat([df1,df2],ignore_index=True,sort=False)

"""The merged data frame is filtered out of duplicate data with respect to Tittle, AppId and Game url using the drop.duplicates"""

rawset=rawset.drop_duplicates(subset=['Titlle','AppID','URL'],keep='first',inplace=False)
rawset=rawset.reset_index(inplace=False, drop=True)
len(rawset)

"""All the features that doesnt quite go along in suggesting the keywords were ignored leaving only the features extracted below."""

data=rawset[['Titlle','AppID','Description','Genre','Score','MinInstalls','Reviews','Free','Currency','Size','Android Version','Genre ID','Version']]

"""Again the filtering is done by removing the duplicates based on AppID."""

data.drop_duplicates(subset='AppID',keep="first",inplace=True)

data.dtypes

"""**DATA VISUALIZATION**

Now that we have gathered all the relevant features necesary for the problem statement, we will further clean the data by analyzing and visualizing the data using plots and get a keen look at the features to decide further and identifying the relationship between the features.
"""

#checking the anamolies in Score column 

sb.boxplot(x=data['Score'])

#deleting the anomalies in the score feature
rawset.loc[np.where(rawset['Score']>5)]

"""Dropping the datapoint that has rating score greater than 5."""

data=data.drop(11299)

"""A histogram is plotted to identify the games rating score and its distribution over the games."""

data.hist(column="Score",figsize=(9,6),bins=20)

"""Providing an index called ID forthe dataframe."""

#changing the index and adding an ID column to this dataframe

data=data.reset_index(inplace=False, drop=True)
data.insert(0, 'ID', range(1, 1 + len(data)))
data

#identifying the genres and grouping them
genre = data[['Genre ID']].groupby(['Genre ID']).sum()

genre.reset_index(inplace = True)
genre.head()

"""Grouping games by genre and plotting to see the detailed veiw of it."""

games_count = {}

# get the list of all genre type counts into a list variable
for i in range(len(data)):
    if data["Genre ID"][i] in games_count.keys():
        games_count[data["Genre ID"][i]] += 1
    else:
        games_count.setdefault(data["Genre ID"][i], 1)

print(games_count)

# seperating the genre names and respective counts into 2 lists 
genre_list = list(games_count.items())
a = []
b = []

for i in range(len(genre_list)):
    a.append(genre_list[i][0])
    b.append(genre_list[i][1])

plt.figure(figsize=(12, 10))
plt.title("The relationship between the genre of games and the number of games per genre")
plt.xlabel("Genre")
plt.ylabel("Number of games per genre")
plt.xticks(rotation=90)
sb.barplot(a, b)
#plt.savefig("genre_list.png")

"""Now to check whether the games are free or not, it is necessary to convert some lower classed words into Upper class words."""

data['Free'] = data['Free'].str.upper()

is_paid=pd.Categorical(data['Free'])
is_paid.describe()

"""Obtaining the minimum, maximum and mean values of the rating score of games and plotted in accordance with the genres."""

game_genre = data[['Genre ID','Score']].groupby(['Genre ID']).agg(["max", "min","mean"])

game_genre.reset_index(inplace = True)
print(game_genre.info())

np.array(game_genre['Score']['max'])

"""Plotting the ratings score with respect to genres."""

# Setting the positions and width for the bars
pos = list(range(len(game_genre['Score']['max']))) 
width = 0.25 
    
# Plotting the bars
fig, ax = plt.subplots(figsize=(20,5))

# Create a bar with pre_score data,
# in position pos,
plt.bar(pos, game_genre['Score']['max'], width, alpha=0.5, color='#EE3224',label=game_genre['Genre ID'][0]) 
plt.bar([p + width for p in pos],  game_genre['Score']['mean'], width, alpha=0.5, color='#F78F1E', label=game_genre['Genre ID'][1]) 
plt.bar([p + width*2 for p in pos], game_genre['Score']['min'], width, alpha=0.5, color='#FFC222', label=game_genre['Genre ID'][2]) 

# Set the y axis label
ax.set_ylabel('Score')
ax.set_title('Genres and respective Scores')

# Set the position of the x ticks
ax.set_xticks([p + 1.5 * width for p in pos])
ax.set_xticklabels(game_genre['Genre ID'], fontweight='bold',rotation='vertical')

plt.xlim(min(pos)-width, max(pos)+width*4)
plt.ylim([0, 2+max(game_genre['Score']['max'])] )

# Adding the legend and showing the plot
plt.legend(['Max', 'Mean', 'Min'], loc='upper left')
plt.grid()
plt.show()

"""By looking at the above plot, it is considered to plot the mean of game ratings score with respect to genres, to check the average mean of the score of games."""

# plt.rcParams.update({'font.size': 8})
plt.figure(figsize= (12,10))
sb.barplot(x =game_genre['Genre ID'], y = game_genre['Score']['mean'],palette="viridis")
plt.title('Mean of Ratings given to games based on genre') 
plt.xticks(rotation=90)   
plt.show()

"""Moving on, the games were estimated with respect to genres and minimum installs of those games. Group by function is used to group the minimum installs by genre id."""

game_genre_installs = data[['Genre ID','MinInstalls']].groupby(['Genre ID']).mean()

game_genre_installs.reset_index(inplace = True)
game_genre_installs['Genre ID']
game_genre_installs=game_genre_installs.sort_values('MinInstalls',ascending=False)

labels = np.array(game_genre_installs['Genre ID'])
sizes = np.array(game_genre_installs['MinInstalls'])

fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
#draw circle
centre_circle = plt.Circle((0,0),0.70,fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)
# Equal aspect ratio ensures that pie is drawn as a circle
ax1.axis('equal')  
plt.title('Mean of minimum installs based on genre')
plt.tight_layout()
plt.show()

"""Different currency options listed for the games in the dataset were listed."""

is_curr=pd.Categorical(data['Currency'])
is_curr.describe()

"""Grouping games by genre and their availability as free or non free versions."""

game_ava = data[['Genre ID','Score','Free']].groupby(['Genre ID','Free']).mean().unstack()
# game_ava.reset_index(inplace = True)
game_ava

game_ava.plot(kind='barh', stacked=True, title='Analyzing game genres distribution w.r.t Free version/paid version ', figsize=[8,10], colormap='winter')

"""Once all the basic visualizations are done, we can see that there exist certain patterns between the features of the dataset. But our main concern is about game description. So further analysis is done on the game description.

**FURTHER TEXT ANALYSIS AND TEXT CLASSIFICATIONS**

To work on the game description and clean it as much as possible certain text analysis techniques specified below were applied to the description feature.
"""

data['Description']  = data['Description'].str.lower()

sam=data['Description'].str.extract(r'(^\w{2})')

print(len(sam))

"""Once converting the description of each game into lower case, the most frquent words used in each game description were extracted to check their frequency in the descriptions."""

# Checking the first 10 most frequent words

cnt = Counter()
for text in data["Description"].values:
    for word in text.split():
        cnt[word] += 1
        
#cnt.most_common(100)

# Removing the frequent words
freq = set([w for (w, wc) in cnt.most_common(10)])
# function to remove the frequent words
def freqwords(text):
    return " ".join([word for word in str(text).split() if word not in freq])

# Passing the function freqwords
data["Description"] = data["Description"].apply(freqwords)
data["Description"].head()

"""Next, we will be removing the urls, web addresses, and emoji's if any exsisting in the game descriptions using the following methods."""

def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

data['Description']=data['Description'].apply(lambda x:remove_urls(x))

"""The beautiful soup library imported is used below to remove the html addresses."""

def remove_html(text):
    return BeautifulSoup(text, "lxml").text

data['Description']=data['Description'].apply(lambda x:remove_html(x))

# 20 rare words in the game description were extracted and deleted 
freq = pd.Series(' '.join(data['Description']).split()).value_counts()[-20:] # 10 rare words
freq = list(freq.index)
data['Description'] = data['Description'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))
data['Description'].head(20)

# Function to remove emoji.
def emoji(string):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  
                           u"\U0001F680-\U0001F6FF"  
                           u"\U0001F1E0-\U0001F1FF"  
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string)

"""The clean description function is defined that contations the methods to remove punctuations, segregating the words into tokens and calling the stopwords to remove essential amount of stopwords. The function is called and applied on the cleaned description."""

def clean_description(text):
  text_nopunctuation= "".join([char for char in text if char not in string.punctuation])
  text_nopunctuation=emoji(text_nopunctuation)

  tokens=word_tokenize(text_nopunctuation)
  # re.split('\W+',text_nopunctuation)
  words = [word for word in tokens if word in all_words or not word.isalpha()]
  text= [word for word in words if word not in stopword]
 
  return text

data['Cleaned_Text']=data['Description'].apply(lambda x:clean_description(x))

data['Cleaned_Text'][:10]

"""As the basic tokenizing and cleaning of the text data is done, lemmatizing the text need to be done to obtain a root word for all similar meaning words. Words need to be lemmatized to group words which possess same meaning, context so that the data available after lemmatizing can be fitted to the model.

The NLTK uses wordnet lemmatizer library and imports the function on to the environment.
"""

wn=nltk.WordNetLemmatizer()
nltk.download('wordnet')

def lemmatizing(tokenized_text):
  text=[wn.lemmatize(word) for word in tokenized_text]
  return text

data['lemmetized_desc']=data['Cleaned_Text'].apply(lambda x: lemmatizing(x))

"""The tokens of words available after all the analysis is done are gruped into sentence or string of values, for further analysis."""

rowitems=[]

for row in data['lemmetized_desc']:
  # row = ' '.join([row])
  rowitems.append(' '.join(row))

data['Cleaned']=rowitems

data['Description_Length']=data['Description'].apply(lambda x: len(x))

data['CleanedDesc_Length']=data['Cleaned'].apply(lambda x: len(x))

"""The original game description and the cleaned text length is documented to check the difference of the length of words. This can help us view that most of the common data in the description were filtered out. Leaving the important stuff."""

def sort_function(matrix):
    tuples = zip(matrix.col, matrix.data)
    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)

"""TF_IDF vectorizer is applied on the clean description to obtain the feature names."""

#METHOD 1

# analyzer=clean_description
tfidf_vect=TfidfVectorizer()
tf_counts=tfidf_vect.fit_transform(data['Cleaned'])
print(tf_counts.shape)
print(tfidf_vect.vocabulary_.keys())

"""Now count vector is applied to eliminate the most repeated words the result is passed to the TF-IDF transformer."""

#METHOD 2

# creating a list of words that eliminates 75% of the most repeated words and keeps only the remaining
cv=CountVectorizer(max_df=0.75,stop_words=stopword)
word_count_vector=cv.fit_transform(data['Cleaned'])

#applying Tf-Idf for the acquired words
tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)
tfidf_transformer.fit(word_count_vector)

"""after transforming the obtained feature names were gathered."""

feature_names=cv.get_feature_names()
len(cv.vocabulary_.keys())

"""We are declaring a method to sort the words of the user given text and to give the  TF-IDF frequency score of top n words in the given text. This method will be called when a user provides his/her text and shows the top n words in his description that are most likely to be kept in his description based on the TF-IDF score."""

# METHOD 3
#take only the feature names and tf-idf score of top n items
def extract_top_vector_keywords(feature_names, all_features, topn=10):
  ten_features=all_features[:topn]
  # to store the top features and their scores  
  scores = []
  features = []
  for index, score in ten_features:
    scores.append(round(score,3))
    features.append(feature_names[index])

  results= {}
  for index in range(len(features)):
      results[features[index]]=scores[index]
    
  return results

"""An other method is declared which states the most rare or unfrequent words available in the user given description with their TF-IDF score."""

#METHOD 4
def extract_rare_keywords(feature_names, all_features, topn=10):
    all_features.reverse()
    del all_features[0:5] # because these are the most frequent common keywords
    ten_features = all_features[:topn]
    # to store the rare features and their scores  
    scores = []
    features = []
    for index, score in ten_features:
        scores.append(round(score, 3))
        features.append(feature_names[index])

    results= {}
    for index in range(len(features)):
        results[features[index]]=scores[index]
    
    return results

"""Here the methods 3 and 4 were called for a user given description."""

#uses method 3 and 4 
def get_tf_idf_predictions(text):
  tf_idf_vector=tfidf_transformer.transform(cv.transform([text]))
  sorted_values=sort_function(tf_idf_vector.tocoo())
  top_keywords=extract_top_vector_keywords(feature_names,sorted_values,12)
  rare_keywords=extract_rare_keywords(feature_names,sorted_values,10)
  return top_keywords,rare_keywords

"""Now we are considering the game description along with title of the game, is rating score and Genre ID. We are passing the features data to the coun vectorizer and then using the TF-IDF transformer to transform the data and get its feature values."""

X=data['Cleaned']+str(data['Score'])+data['Titlle']+str(data['Genre ID'])

cv2=CountVectorizer(max_df=0.75,stop_words=stopword)
word_count_vector2=cv2.fit_transform(X)

tfidf_transformer2=TfidfTransformer(smooth_idf=True,use_idf=True)
tfidf_transformer2.fit(word_count_vector2)
all_features=cv2.get_feature_names()
len(cv2.vocabulary_.keys())

"""Below we are calling the methods that are used to extract top n words and its tf-idf score along with rare words and their tf-idf score for a user  given text considering the genre Id, title, score and description of the dataset."""

#uses method 3 and 4 for the transformed text with respect to genre, score, title, description
def get_genre_based_predictions(text):
  vectors=tfidf_transformer2.transform(cv2.transform([text]))
  sorted_values=sort_function(vectors.tocoo())
  top_keywords=extract_top_vector_keywords(all_features,sorted_values,15)
  rare_keywords=extract_rare_keywords(all_features,sorted_values,10)
  return top_keywords,rare_keywords

def print_function(x):
  for k in x:
    value=x[k]
    print(k +": ",round(value*100,4))

"""The above method is used to obtaine a rounded percentage value of the tf-idf score for each word.

**MODEL IMPLEMENTATION**

Now for the K-Means clustering model we are using Mini Batch K-Means to attain the clusters.
"""

def find_optimal_clusters(data, max_k):
    iters = range(2, max_k+1, 2)
    
    sse = []
    for k in iters:
        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)
        # sse.append(MiniBatchKMeans(n_clusters=k, random_state=20).fit(data).inertia_)
        print('Fit {} clusters'.format(k))
        
    f, ax = plt.subplots(1, 1)
    ax.plot(iters, sse, marker='o')
    ax.set_xlabel('Cluster Centers')
    ax.set_xticks(iters)
    ax.set_xticklabels(iters)
    ax.set_ylabel('SSE')
    ax.set_title('SSE by Cluster Center Plot (Elbow Method)')

"""The above defined method is called by feeding the model with TF-IDF vectorized data, but not the transformed data, The transformed data is only used to get the most important and rare words of the user given description with tf-idf score. 
Here for the model to form cluster only vectorized data (tf_count) is passed.
"""

find_optimal_clusters(tf_counts, 40) # based on only game description that is cleaned

# Commented out IPython magic to ensure Python compatibility.
#following are used for keywords prediction through CLUSTERING done based on only game descriptions

true_k = 16
k_model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)
# %time k_model.fit(tf_counts)

order_centroids = k_model.cluster_centers_.argsort()[:, ::-1]
terms = tfidf_vect.get_feature_names()

def display_top_keywords(true_k,words_count):
  for i in range(true_k):
    print("\nCluster %d: " % i),
    for indexword in order_centroids[i, :words_count]:
      print("%s" % terms[indexword])

"""Above we defining a method that can be callable to display the top n keywords of the predicted cluster based on user description.

And below is the method to get the cluster keywords generated.
"""

def display_cluster_keywords(cluster_number,words_count):
  cluster_keywords=[]
  for indexword in order_centroids[cluster_number, :words_count]:
    cluster_keywords.append(terms[indexword])
    # print("%s" % terms[indexword])
  return cluster_keywords

"""The rare or most unlikely words that are not to be added to the description are extracted below."""

def display_cluster_rare_words(cluster_number,words_count):
  cluster_keywords=[]
  rare_words_centroids=order_centroids[::-1] # reverse the array
  for indexword in rare_words_centroids[cluster_number, :words_count]:
    cluster_keywords.append(terms[indexword])
    # print("%s" % terms[indexword])
  return cluster_keywords

"""Base on the trained data the model will be predicting a cluster in which the example user description falls into. The predicted clusteris gathered along with the cluster words and rare words that are defined in above method."""

# METHOD 6
def get_cluster_predictions(text):
  test_X = tfidf_vect.transform([text])
  predicted = k_model.predict(test_X)
  cluster_topwords=display_cluster_keywords(int(predicted),20)
  cluster_rarewords=display_cluster_rare_words(int(predicted),10)
  test_tokens=re.split('\W+',text)
  suggest_words=[i for i in test_tokens + cluster_topwords if i not in test_tokens ] ## list1 - list2
  must_words=[i for i in cluster_topwords if i not in suggest_words]
  return predicted, must_words,suggest_words,cluster_rarewords

"""Below we are providing an example game description to texst the model."""

#give the description that has to be predicted
test_description='Escape to the world of farming, friends and fun! Go on farm adventures to collect rare goods and craft new recipes. Raise animals and grow your farm with friends. Join a farm Co-Op to trade and share or play on your own in Anonymous Mode. You can play FarmVille anytime, anywhere… even when not connected to the internet. Best of all, the world’s most popular farming game is free to play'

"""For the given example description, the model geerated cluster in which the description falls into, the keywords of that cluster, the words that need to be added to the description apart from the existing ones and finally the words that shouldnt be added to description were printed out over here using the method get cluster prediction defined above as method 6."""

x,y,z,w=get_cluster_predictions(test_description)
print(x)
print(y)
print(z)
print(w)

"""All the results generated were clearly defined by declaring a method that calls all the methods defined above and printed  for a breif understanding."""

def all_suggestions(text):
  a,b,c,d=get_cluster_predictions(text)
  e,f=get_tf_idf_predictions(text)
  g,h=get_genre_based_predictions(text)
  print("Predicted Cluster : ",a)
  print("\nWords that are must to be kept in description : ",b)
  print("\nWords to be Added (Suggestion from Clustering model) :",c)
  print("\nWords not to be Added (Suggestion from Clustering model) :",d)
  print("\nTF-IDF score determinations from trained Game Descriptions")
  #KEYWORDS that are likely to be expected from this given type of game
  print("Keywords in the given description that are more likely interested to keep: \n")
  print_function(e)
  #KEYWORDS that are likely to be expected from this given type of game
  print("\nKeywords in the given description that are not very likely interested to keep: \n")
  print_function(f)

  print("\nTF-IDF score determinations from trained Game Descriptions+Genre+Title+Score/Rating")
  print("Keywords in the given description that are more likely interested to keep based on the Genre and Score Ratings: \n")
  print_function(g)
  print("\nKeywords in the given description that are not very likely interested to keep based on the Genre and Score Ratings:")
  print_function(h)

"""The all suggestion method is applied to the exaample description that is given above and the outputs were printed."""

all_suggestions(test_description)